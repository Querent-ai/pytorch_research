{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM0MatCYATmKG+bNsA2fuz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Querent-ai/pytorch_research/blob/main/Facies_weighted_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Facies Prediction with Class Weighting & Visualization](https://)**"
      ],
      "metadata": {
        "id": "kKGCvY9uek9p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v05z_ETQeh3y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"mcmurray_facies_v1.csv\")  # Change path as needed\n",
        "df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
        "df = df.dropna(subset=['GR', 'RHOB', 'NPHI', 'PHI', 'VSH', 'lithName'])  # Drop rows missing features/target\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIP0GCQce1qE",
        "outputId": "70833950-5075-46b4-a2fc-1d75df9a1bf3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-8f28c2b40e49>:1: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"mcmurray_facies_v1.csv\")  # Change path as needed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['GR', 'RHOB', 'NPHI', 'PHI', 'VSH']\n",
        "target = 'lithName'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n"
      ],
      "metadata": {
        "id": "XS-YUVhynEgQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n"
      ],
      "metadata": {
        "id": "KLKCWjbvoJJh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = Counter(y_encoded)\n",
        "valid_labels = [label for label, count in counts.items() if count >= 2]\n",
        "mask = np.isin(y_encoded, valid_labels)\n",
        "\n",
        "X_filtered = X_scaled[mask]\n",
        "y_filtered = y_encoded[mask]\n",
        "\n",
        "# Re-encode filtered labels for consistency\n",
        "le_filtered = LabelEncoder()\n",
        "y_filtered = le_filtered.fit_transform(y_filtered)\n",
        "classes = le_filtered.classes_\n"
      ],
      "metadata": {
        "id": "EiuimQQBoMqp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered\n",
        ")\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "Q4Uf6FhIoPdp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FaciesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "train_loader = DataLoader(FaciesDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(FaciesDataset(X_test, y_test), batch_size=64)\n"
      ],
      "metadata": {
        "id": "bWfmjswkoctc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FaciesMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = FaciesMLP(input_dim=len(features), hidden_dim=64, output_dim=len(classes))\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "FP6ijhxOohVq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5E6pq3TolWE",
        "outputId": "93fcd6f4-f90a-4fb5-d52b-44c126755950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.3405\n",
            "Epoch 2, Loss: 0.3141\n",
            "Epoch 3, Loss: 0.3141\n",
            "Epoch 4, Loss: 0.3032\n",
            "Epoch 5, Loss: 0.3038\n",
            "Epoch 6, Loss: 0.2989\n",
            "Epoch 7, Loss: 0.2813\n",
            "Epoch 8, Loss: 0.2704\n",
            "Epoch 9, Loss: 0.2683\n",
            "Epoch 10, Loss: 0.2611\n",
            "Epoch 11, Loss: 0.2697\n",
            "Epoch 12, Loss: 0.2572\n",
            "Epoch 13, Loss: 0.2502\n",
            "Epoch 14, Loss: 0.2544\n",
            "Epoch 15, Loss: 0.2468\n",
            "Epoch 16, Loss: 0.2331\n",
            "Epoch 17, Loss: 0.2290\n",
            "Epoch 18, Loss: 0.2324\n",
            "Epoch 19, Loss: 0.2150\n",
            "Epoch 20, Loss: 0.2143\n",
            "Epoch 21, Loss: 0.2109\n",
            "Epoch 22, Loss: 0.2142\n",
            "Epoch 23, Loss: 0.2136\n",
            "Epoch 24, Loss: 0.2105\n",
            "Epoch 25, Loss: 0.2216\n",
            "Epoch 26, Loss: 0.1985\n",
            "Epoch 27, Loss: 0.1948\n",
            "Epoch 28, Loss: 0.2039\n",
            "Epoch 29, Loss: 0.2005\n",
            "Epoch 30, Loss: 0.1928\n",
            "Epoch 31, Loss: 0.1867\n",
            "Epoch 32, Loss: 0.1944\n",
            "Epoch 33, Loss: 0.1895\n",
            "Epoch 34, Loss: 0.1906\n",
            "Epoch 35, Loss: 0.1865\n",
            "Epoch 36, Loss: 0.1799\n",
            "Epoch 37, Loss: 0.1776\n",
            "Epoch 38, Loss: 0.1857\n",
            "Epoch 39, Loss: 0.1831\n",
            "Epoch 40, Loss: 0.1763\n",
            "Epoch 41, Loss: 0.1756\n",
            "Epoch 42, Loss: 0.1779\n",
            "Epoch 43, Loss: 0.1666\n",
            "Epoch 44, Loss: 0.1708\n",
            "Epoch 45, Loss: 0.1686\n",
            "Epoch 46, Loss: 0.1615\n",
            "Epoch 47, Loss: 0.1679\n",
            "Epoch 48, Loss: 0.1705\n",
            "Epoch 49, Loss: 0.1793\n",
            "Epoch 50, Loss: 0.1625\n",
            "Epoch 51, Loss: 0.1597\n",
            "Epoch 52, Loss: 0.1755\n",
            "Epoch 53, Loss: 0.1551\n",
            "Epoch 54, Loss: 0.1511\n",
            "Epoch 55, Loss: 0.1591\n",
            "Epoch 56, Loss: 0.1616\n",
            "Epoch 57, Loss: 0.1535\n",
            "Epoch 58, Loss: 0.1557\n",
            "Epoch 59, Loss: 0.1523\n",
            "Epoch 60, Loss: 0.1471\n",
            "Epoch 61, Loss: 0.1454\n",
            "Epoch 62, Loss: 0.1471\n",
            "Epoch 63, Loss: 0.1529\n",
            "Epoch 64, Loss: 0.1470\n",
            "Epoch 65, Loss: 0.1413\n",
            "Epoch 66, Loss: 0.1606\n",
            "Epoch 67, Loss: 0.1519\n",
            "Epoch 68, Loss: 0.1529\n",
            "Epoch 69, Loss: 0.1426\n",
            "Epoch 70, Loss: 0.1499\n",
            "Epoch 71, Loss: 0.1430\n",
            "Epoch 72, Loss: 0.1435\n",
            "Epoch 73, Loss: 0.1397\n",
            "Epoch 74, Loss: 0.1434\n",
            "Epoch 75, Loss: 0.1438\n",
            "Epoch 76, Loss: 0.1450\n",
            "Epoch 77, Loss: 0.1438\n",
            "Epoch 78, Loss: 0.1461\n",
            "Epoch 79, Loss: 0.1345\n",
            "Epoch 80, Loss: 0.1417\n",
            "Epoch 81, Loss: 0.1401\n",
            "Epoch 82, Loss: 0.1412\n",
            "Epoch 83, Loss: 0.1382\n",
            "Epoch 84, Loss: 0.1343\n",
            "Epoch 85, Loss: 0.1316\n",
            "Epoch 86, Loss: 0.1389\n",
            "Epoch 87, Loss: 0.1312\n",
            "Epoch 88, Loss: 0.1318\n",
            "Epoch 89, Loss: 0.1369\n",
            "Epoch 90, Loss: 0.1334\n",
            "Epoch 91, Loss: 0.1308\n",
            "Epoch 92, Loss: 0.1293\n",
            "Epoch 93, Loss: 0.1258\n",
            "Epoch 94, Loss: 0.1288\n",
            "Epoch 95, Loss: 0.1340\n",
            "Epoch 96, Loss: 0.1355\n",
            "Epoch 97, Loss: 0.1237\n",
            "Epoch 98, Loss: 0.1298\n",
            "Epoch 99, Loss: 0.1337\n",
            "Epoch 100, Loss: 0.1282\n",
            "Epoch 101, Loss: 0.1254\n",
            "Epoch 102, Loss: 0.1365\n",
            "Epoch 103, Loss: 0.1218\n",
            "Epoch 104, Loss: 0.1242\n",
            "Epoch 105, Loss: 0.1193\n",
            "Epoch 106, Loss: 0.1233\n",
            "Epoch 107, Loss: 0.1279\n",
            "Epoch 108, Loss: 0.1227\n",
            "Epoch 109, Loss: 0.1220\n",
            "Epoch 110, Loss: 0.1202\n",
            "Epoch 111, Loss: 0.1247\n",
            "Epoch 112, Loss: 0.1219\n",
            "Epoch 113, Loss: 0.1217\n",
            "Epoch 114, Loss: 0.1223\n",
            "Epoch 115, Loss: 0.1221\n",
            "Epoch 116, Loss: 0.1267\n",
            "Epoch 117, Loss: 0.1273\n",
            "Epoch 118, Loss: 0.1235\n",
            "Epoch 119, Loss: 0.1223\n",
            "Epoch 120, Loss: 0.1161\n",
            "Epoch 121, Loss: 0.1319\n",
            "Epoch 122, Loss: 0.1198\n",
            "Epoch 123, Loss: 0.1102\n",
            "Epoch 124, Loss: 0.1244\n",
            "Epoch 125, Loss: 0.1111\n",
            "Epoch 126, Loss: 0.1155\n",
            "Epoch 127, Loss: 0.1285\n",
            "Epoch 128, Loss: 0.1198\n",
            "Epoch 129, Loss: 0.1220\n",
            "Epoch 130, Loss: 0.1140\n",
            "Epoch 131, Loss: 0.1168\n",
            "Epoch 132, Loss: 0.1220\n",
            "Epoch 133, Loss: 0.1246\n",
            "Epoch 134, Loss: 0.1141\n",
            "Epoch 135, Loss: 0.1224\n",
            "Epoch 136, Loss: 0.1138\n",
            "Epoch 137, Loss: 0.1141\n",
            "Epoch 138, Loss: 0.1143\n",
            "Epoch 139, Loss: 0.1165\n",
            "Epoch 140, Loss: 0.1130\n",
            "Epoch 141, Loss: 0.1122\n",
            "Epoch 142, Loss: 0.1105\n",
            "Epoch 143, Loss: 0.1180\n",
            "Epoch 144, Loss: 0.1122\n",
            "Epoch 145, Loss: 0.1178\n",
            "Epoch 146, Loss: 0.1131\n",
            "Epoch 147, Loss: 0.1115\n",
            "Epoch 148, Loss: 0.1167\n",
            "Epoch 149, Loss: 0.1082\n",
            "Epoch 150, Loss: 0.1140\n",
            "Epoch 151, Loss: 0.1124\n",
            "Epoch 152, Loss: 0.1200\n",
            "Epoch 153, Loss: 0.1005\n",
            "Epoch 154, Loss: 0.1094\n",
            "Epoch 155, Loss: 0.1116\n",
            "Epoch 156, Loss: 0.1182\n",
            "Epoch 157, Loss: 0.1106\n",
            "Epoch 158, Loss: 0.1117\n",
            "Epoch 159, Loss: 0.1111\n",
            "Epoch 160, Loss: 0.1080\n",
            "Epoch 161, Loss: 0.1166\n",
            "Epoch 162, Loss: 0.1050\n",
            "Epoch 163, Loss: 0.1090\n",
            "Epoch 164, Loss: 0.1116\n",
            "Epoch 165, Loss: 0.1140\n",
            "Epoch 166, Loss: 0.1098\n",
            "Epoch 167, Loss: 0.1033\n",
            "Epoch 168, Loss: 0.1079\n",
            "Epoch 169, Loss: 0.1112\n",
            "Epoch 170, Loss: 0.1072\n",
            "Epoch 171, Loss: 0.1137\n",
            "Epoch 172, Loss: 0.1019\n",
            "Epoch 173, Loss: 0.1045\n",
            "Epoch 174, Loss: 0.1010\n",
            "Epoch 175, Loss: 0.1099\n",
            "Epoch 176, Loss: 0.1013\n",
            "Epoch 177, Loss: 0.1081\n",
            "Epoch 178, Loss: 0.1098\n",
            "Epoch 179, Loss: 0.1116\n",
            "Epoch 180, Loss: 0.1029\n",
            "Epoch 181, Loss: 0.0984\n",
            "Epoch 182, Loss: 0.1037\n",
            "Epoch 183, Loss: 0.1148\n",
            "Epoch 184, Loss: 0.1020\n",
            "Epoch 185, Loss: 0.1037\n",
            "Epoch 186, Loss: 0.1027\n",
            "Epoch 187, Loss: 0.1053\n",
            "Epoch 188, Loss: 0.0991\n",
            "Epoch 189, Loss: 0.1008\n",
            "Epoch 190, Loss: 0.1007\n",
            "Epoch 191, Loss: 0.1017\n",
            "Epoch 192, Loss: 0.1019\n",
            "Epoch 193, Loss: 0.1017\n",
            "Epoch 194, Loss: 0.1054\n",
            "Epoch 195, Loss: 0.1034\n",
            "Epoch 196, Loss: 0.1050\n",
            "Epoch 197, Loss: 0.0977\n",
            "Epoch 198, Loss: 0.1004\n",
            "Epoch 199, Loss: 0.1028\n",
            "Epoch 200, Loss: 0.0998\n",
            "Epoch 201, Loss: 0.1023\n",
            "Epoch 202, Loss: 0.1035\n",
            "Epoch 203, Loss: 0.1038\n",
            "Epoch 204, Loss: 0.0954\n",
            "Epoch 205, Loss: 0.1017\n",
            "Epoch 206, Loss: 0.0956\n",
            "Epoch 207, Loss: 0.1064\n",
            "Epoch 208, Loss: 0.1016\n",
            "Epoch 209, Loss: 0.0987\n",
            "Epoch 210, Loss: 0.1026\n",
            "Epoch 211, Loss: 0.1016\n",
            "Epoch 212, Loss: 0.1034\n",
            "Epoch 213, Loss: 0.1030\n",
            "Epoch 214, Loss: 0.1035\n",
            "Epoch 215, Loss: 0.1014\n",
            "Epoch 216, Loss: 0.1052\n",
            "Epoch 217, Loss: 0.1043\n",
            "Epoch 218, Loss: 0.1017\n",
            "Epoch 219, Loss: 0.0976\n",
            "Epoch 220, Loss: 0.1066\n",
            "Epoch 221, Loss: 0.1010\n",
            "Epoch 222, Loss: 0.0992\n",
            "Epoch 223, Loss: 0.0992\n",
            "Epoch 224, Loss: 0.0999\n",
            "Epoch 225, Loss: 0.0990\n",
            "Epoch 226, Loss: 0.1003\n",
            "Epoch 227, Loss: 0.0993\n",
            "Epoch 228, Loss: 0.0955\n",
            "Epoch 229, Loss: 0.0981\n",
            "Epoch 230, Loss: 0.0996\n",
            "Epoch 231, Loss: 0.0951\n",
            "Epoch 232, Loss: 0.1069\n",
            "Epoch 233, Loss: 0.0998\n",
            "Epoch 234, Loss: 0.0937\n",
            "Epoch 235, Loss: 0.0934\n",
            "Epoch 236, Loss: 0.1007\n",
            "Epoch 237, Loss: 0.1062\n",
            "Epoch 238, Loss: 0.1044\n",
            "Epoch 239, Loss: 0.1010\n",
            "Epoch 240, Loss: 0.0986\n",
            "Epoch 241, Loss: 0.0957\n",
            "Epoch 242, Loss: 0.0995\n",
            "Epoch 243, Loss: 0.0960\n",
            "Epoch 244, Loss: 0.0979\n",
            "Epoch 245, Loss: 0.1036\n",
            "Epoch 246, Loss: 0.0955\n",
            "Epoch 247, Loss: 0.0947\n",
            "Epoch 248, Loss: 0.1002\n",
            "Epoch 249, Loss: 0.0924\n",
            "Epoch 250, Loss: 0.0974\n",
            "Epoch 251, Loss: 0.0893\n",
            "Epoch 252, Loss: 0.0979\n",
            "Epoch 253, Loss: 0.0907\n",
            "Epoch 254, Loss: 0.0945\n",
            "Epoch 255, Loss: 0.0902\n",
            "Epoch 256, Loss: 0.0947\n",
            "Epoch 257, Loss: 0.0981\n",
            "Epoch 258, Loss: 0.0971\n",
            "Epoch 259, Loss: 0.0980\n",
            "Epoch 260, Loss: 0.0948\n",
            "Epoch 261, Loss: 0.0918\n",
            "Epoch 262, Loss: 0.0969\n",
            "Epoch 263, Loss: 0.0936\n",
            "Epoch 264, Loss: 0.1029\n",
            "Epoch 265, Loss: 0.0901\n",
            "Epoch 266, Loss: 0.1038\n",
            "Epoch 267, Loss: 0.0968\n",
            "Epoch 268, Loss: 0.0928\n",
            "Epoch 269, Loss: 0.0964\n",
            "Epoch 270, Loss: 0.1070\n",
            "Epoch 271, Loss: 0.0928\n",
            "Epoch 272, Loss: 0.0969\n",
            "Epoch 273, Loss: 0.0920\n",
            "Epoch 274, Loss: 0.0931\n",
            "Epoch 275, Loss: 0.0916\n",
            "Epoch 276, Loss: 0.0888\n",
            "Epoch 277, Loss: 0.0975\n",
            "Epoch 278, Loss: 0.0932\n",
            "Epoch 279, Loss: 0.0946\n",
            "Epoch 280, Loss: 0.0940\n",
            "Epoch 281, Loss: 0.0907\n",
            "Epoch 282, Loss: 0.0950\n",
            "Epoch 283, Loss: 0.0952\n",
            "Epoch 284, Loss: 0.0891\n",
            "Epoch 285, Loss: 0.0943\n",
            "Epoch 286, Loss: 0.0911\n",
            "Epoch 287, Loss: 0.0896\n",
            "Epoch 288, Loss: 0.0978\n",
            "Epoch 289, Loss: 0.0899\n",
            "Epoch 290, Loss: 0.0917\n",
            "Epoch 291, Loss: 0.0886\n",
            "Epoch 292, Loss: 0.0903\n",
            "Epoch 293, Loss: 0.0862\n",
            "Epoch 294, Loss: 0.1002\n",
            "Epoch 295, Loss: 0.0941\n",
            "Epoch 296, Loss: 0.0876\n",
            "Epoch 297, Loss: 0.0950\n",
            "Epoch 298, Loss: 0.0966\n",
            "Epoch 299, Loss: 0.0924\n",
            "Epoch 300, Loss: 0.0924\n",
            "Epoch 301, Loss: 0.0921\n",
            "Epoch 302, Loss: 0.0918\n",
            "Epoch 303, Loss: 0.0923\n",
            "Epoch 304, Loss: 0.0902\n",
            "Epoch 305, Loss: 0.0893\n",
            "Epoch 306, Loss: 0.0903\n",
            "Epoch 307, Loss: 0.0917\n",
            "Epoch 308, Loss: 0.0888\n",
            "Epoch 309, Loss: 0.0881\n",
            "Epoch 310, Loss: 0.0890\n",
            "Epoch 311, Loss: 0.0885\n",
            "Epoch 312, Loss: 0.0900\n",
            "Epoch 313, Loss: 0.0870\n",
            "Epoch 314, Loss: 0.0895\n",
            "Epoch 315, Loss: 0.0894\n",
            "Epoch 316, Loss: 0.0906\n",
            "Epoch 317, Loss: 0.0867\n",
            "Epoch 318, Loss: 0.0936\n",
            "Epoch 319, Loss: 0.0836\n",
            "Epoch 320, Loss: 0.0952\n",
            "Epoch 321, Loss: 0.0915\n",
            "Epoch 322, Loss: 0.0882\n",
            "Epoch 323, Loss: 0.0895\n",
            "Epoch 324, Loss: 0.0880\n",
            "Epoch 325, Loss: 0.0911\n",
            "Epoch 326, Loss: 0.0845\n",
            "Epoch 327, Loss: 0.0860\n",
            "Epoch 328, Loss: 0.0836\n",
            "Epoch 329, Loss: 0.0871\n",
            "Epoch 330, Loss: 0.0879\n",
            "Epoch 331, Loss: 0.0905\n",
            "Epoch 332, Loss: 0.0938\n",
            "Epoch 333, Loss: 0.0858\n",
            "Epoch 334, Loss: 0.0860\n",
            "Epoch 335, Loss: 0.0897\n",
            "Epoch 336, Loss: 0.0814\n",
            "Epoch 337, Loss: 0.0941\n",
            "Epoch 338, Loss: 0.0873\n",
            "Epoch 339, Loss: 0.0884\n",
            "Epoch 340, Loss: 0.0870\n",
            "Epoch 341, Loss: 0.0904\n",
            "Epoch 342, Loss: 0.0916\n",
            "Epoch 343, Loss: 0.0864\n",
            "Epoch 344, Loss: 0.0893\n",
            "Epoch 345, Loss: 0.0864\n",
            "Epoch 346, Loss: 0.0906\n",
            "Epoch 347, Loss: 0.0920\n",
            "Epoch 348, Loss: 0.0856\n",
            "Epoch 349, Loss: 0.0894\n",
            "Epoch 350, Loss: 0.0890\n",
            "Epoch 351, Loss: 0.0889\n",
            "Epoch 352, Loss: 0.0856\n",
            "Epoch 353, Loss: 0.0894\n",
            "Epoch 354, Loss: 0.0820\n",
            "Epoch 355, Loss: 0.0880\n",
            "Epoch 356, Loss: 0.0906\n",
            "Epoch 357, Loss: 0.0921\n",
            "Epoch 358, Loss: 0.0869\n",
            "Epoch 359, Loss: 0.0904\n",
            "Epoch 360, Loss: 0.0809\n",
            "Epoch 361, Loss: 0.0876\n",
            "Epoch 362, Loss: 0.0839\n",
            "Epoch 363, Loss: 0.0792\n",
            "Epoch 364, Loss: 0.0850\n",
            "Epoch 365, Loss: 0.0848\n",
            "Epoch 366, Loss: 0.0841\n",
            "Epoch 367, Loss: 0.0894\n",
            "Epoch 368, Loss: 0.0904\n",
            "Epoch 369, Loss: 0.0835\n",
            "Epoch 370, Loss: 0.0904\n",
            "Epoch 371, Loss: 0.0898\n",
            "Epoch 372, Loss: 0.0837\n",
            "Epoch 373, Loss: 0.0915\n",
            "Epoch 374, Loss: 0.0875\n",
            "Epoch 375, Loss: 0.0816\n",
            "Epoch 376, Loss: 0.0882\n",
            "Epoch 377, Loss: 0.0860\n",
            "Epoch 378, Loss: 0.0893\n",
            "Epoch 379, Loss: 0.0922\n",
            "Epoch 380, Loss: 0.0851\n",
            "Epoch 381, Loss: 0.0846\n",
            "Epoch 382, Loss: 0.0811\n",
            "Epoch 383, Loss: 0.0842\n",
            "Epoch 384, Loss: 0.0909\n",
            "Epoch 385, Loss: 0.0854\n",
            "Epoch 386, Loss: 0.0889\n",
            "Epoch 387, Loss: 0.0818\n",
            "Epoch 388, Loss: 0.0826\n",
            "Epoch 389, Loss: 0.0808\n",
            "Epoch 390, Loss: 0.0882\n",
            "Epoch 391, Loss: 0.0846\n",
            "Epoch 392, Loss: 0.0837\n",
            "Epoch 393, Loss: 0.0876\n",
            "Epoch 394, Loss: 0.0832\n",
            "Epoch 395, Loss: 0.0865\n",
            "Epoch 396, Loss: 0.0855\n",
            "Epoch 397, Loss: 0.0797\n",
            "Epoch 398, Loss: 0.0784\n",
            "Epoch 399, Loss: 0.0868\n",
            "Epoch 400, Loss: 0.0869\n",
            "Epoch 401, Loss: 0.0837\n",
            "Epoch 402, Loss: 0.0850\n",
            "Epoch 403, Loss: 0.0861\n",
            "Epoch 404, Loss: 0.0886\n",
            "Epoch 405, Loss: 0.0898\n",
            "Epoch 406, Loss: 0.0789\n",
            "Epoch 407, Loss: 0.0819\n",
            "Epoch 408, Loss: 0.0861\n",
            "Epoch 409, Loss: 0.0776\n",
            "Epoch 410, Loss: 0.0861\n",
            "Epoch 411, Loss: 0.0833\n",
            "Epoch 412, Loss: 0.0834\n",
            "Epoch 413, Loss: 0.0773\n",
            "Epoch 414, Loss: 0.0871\n",
            "Epoch 415, Loss: 0.0917\n",
            "Epoch 416, Loss: 0.0790\n",
            "Epoch 417, Loss: 0.0812\n",
            "Epoch 418, Loss: 0.0904\n",
            "Epoch 419, Loss: 0.0826\n",
            "Epoch 420, Loss: 0.0806\n",
            "Epoch 421, Loss: 0.0800\n",
            "Epoch 422, Loss: 0.0862\n",
            "Epoch 423, Loss: 0.0867\n",
            "Epoch 424, Loss: 0.0805\n",
            "Epoch 425, Loss: 0.0819\n",
            "Epoch 426, Loss: 0.0828\n",
            "Epoch 427, Loss: 0.0898\n",
            "Epoch 428, Loss: 0.0812\n",
            "Epoch 429, Loss: 0.0819\n",
            "Epoch 430, Loss: 0.0760\n",
            "Epoch 431, Loss: 0.0831\n",
            "Epoch 432, Loss: 0.0798\n",
            "Epoch 433, Loss: 0.0805\n",
            "Epoch 434, Loss: 0.0852\n",
            "Epoch 435, Loss: 0.0818\n",
            "Epoch 436, Loss: 0.0860\n",
            "Epoch 437, Loss: 0.0862\n",
            "Epoch 438, Loss: 0.0850\n",
            "Epoch 439, Loss: 0.0804\n",
            "Epoch 440, Loss: 0.0827\n",
            "Epoch 441, Loss: 0.0803\n",
            "Epoch 442, Loss: 0.0844\n",
            "Epoch 443, Loss: 0.0826\n",
            "Epoch 444, Loss: 0.0836\n",
            "Epoch 445, Loss: 0.0822\n",
            "Epoch 446, Loss: 0.0815\n",
            "Epoch 447, Loss: 0.0823\n",
            "Epoch 448, Loss: 0.0807\n",
            "Epoch 449, Loss: 0.0852\n",
            "Epoch 450, Loss: 0.0820\n",
            "Epoch 451, Loss: 0.0730\n",
            "Epoch 452, Loss: 0.0876\n",
            "Epoch 453, Loss: 0.0814\n",
            "Epoch 454, Loss: 0.0837\n",
            "Epoch 455, Loss: 0.0792\n",
            "Epoch 456, Loss: 0.0807\n",
            "Epoch 457, Loss: 0.0749\n",
            "Epoch 458, Loss: 0.0794\n",
            "Epoch 459, Loss: 0.0816\n",
            "Epoch 460, Loss: 0.0781\n",
            "Epoch 461, Loss: 0.0824\n",
            "Epoch 462, Loss: 0.0810\n",
            "Epoch 463, Loss: 0.0732\n",
            "Epoch 464, Loss: 0.0843\n",
            "Epoch 465, Loss: 0.0779\n",
            "Epoch 466, Loss: 0.0796\n",
            "Epoch 467, Loss: 0.0811\n",
            "Epoch 468, Loss: 0.0819\n",
            "Epoch 469, Loss: 0.0834\n",
            "Epoch 470, Loss: 0.0785\n",
            "Epoch 471, Loss: 0.0725\n",
            "Epoch 472, Loss: 0.0804\n",
            "Epoch 473, Loss: 0.0798\n",
            "Epoch 474, Loss: 0.0845\n",
            "Epoch 475, Loss: 0.0812\n",
            "Epoch 476, Loss: 0.0739\n",
            "Epoch 477, Loss: 0.0841\n",
            "Epoch 478, Loss: 0.0768\n",
            "Epoch 479, Loss: 0.0791\n",
            "Epoch 480, Loss: 0.0774\n",
            "Epoch 481, Loss: 0.0871\n",
            "Epoch 482, Loss: 0.0816\n",
            "Epoch 483, Loss: 0.0763\n",
            "Epoch 484, Loss: 0.0848\n",
            "Epoch 485, Loss: 0.0781\n",
            "Epoch 486, Loss: 0.0796\n",
            "Epoch 487, Loss: 0.0823\n",
            "Epoch 488, Loss: 0.0839\n",
            "Epoch 489, Loss: 0.0844\n",
            "Epoch 490, Loss: 0.0864\n",
            "Epoch 491, Loss: 0.0814\n",
            "Epoch 492, Loss: 0.0798\n",
            "Epoch 493, Loss: 0.0830\n",
            "Epoch 494, Loss: 0.0739\n",
            "Epoch 495, Loss: 0.0843\n",
            "Epoch 496, Loss: 0.0807\n",
            "Epoch 497, Loss: 0.0775\n",
            "Epoch 498, Loss: 0.0795\n",
            "Epoch 499, Loss: 0.0773\n",
            "Epoch 500, Loss: 0.0806\n",
            "Epoch 501, Loss: 0.0800\n",
            "Epoch 502, Loss: 0.0755\n",
            "Epoch 503, Loss: 0.0764\n",
            "Epoch 504, Loss: 0.0766\n",
            "Epoch 505, Loss: 0.0820\n",
            "Epoch 506, Loss: 0.0741\n",
            "Epoch 507, Loss: 0.0805\n",
            "Epoch 508, Loss: 0.0815\n",
            "Epoch 509, Loss: 0.0814\n",
            "Epoch 510, Loss: 0.0833\n",
            "Epoch 511, Loss: 0.0777\n",
            "Epoch 512, Loss: 0.0742\n",
            "Epoch 513, Loss: 0.0762\n",
            "Epoch 514, Loss: 0.0821\n",
            "Epoch 515, Loss: 0.0827\n",
            "Epoch 516, Loss: 0.0837\n",
            "Epoch 517, Loss: 0.0765\n",
            "Epoch 518, Loss: 0.0861\n",
            "Epoch 519, Loss: 0.0760\n",
            "Epoch 520, Loss: 0.0746\n",
            "Epoch 521, Loss: 0.0771\n",
            "Epoch 522, Loss: 0.0832\n",
            "Epoch 523, Loss: 0.0887\n",
            "Epoch 524, Loss: 0.0781\n",
            "Epoch 525, Loss: 0.0784\n",
            "Epoch 526, Loss: 0.0738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        out = model(xb)\n",
        "        preds = torch.argmax(out, dim=1)\n",
        "        all_preds.extend(preds.numpy())\n",
        "        all_labels.extend(yb.numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, target_names=classes, zero_division=0))\n",
        "cm = confusion_matrix(all_labels, all_preds)\n"
      ],
      "metadata": {
        "id": "bTJd27YAoqC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Facies Confusion Matrix (Filtered Classes)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l_o02d-wouHj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}